<!DOCTYPE html>
<html>
    <head>
        <title>Convolutional neural networks for detection of hand-written drawings</title>    
    </head>

    <style>
        table{
            width: 100%;
            table-layout: fixed;
        }

        td{
            padding: 10px;
            vertical-align: top;
            text-align: justify;
        }

        td.left, td.right{
            width: 50%;
        }

        h1{
            text-align: center;
        }

        .centrado{
            text-align: center;
        }
    </style>

    <body>
        <h1 style="text-align: center;">Convolutional neural networks for detection of hand-written drawings</h1>

        <table>
            <tbody>
                <tr>
                    <td>Sergio E. Valenzuela
                        Universidad Panamericana
                        Facultad de Ingenier√≠a
                        Aguascalientes, M√©xico
                        sevalenzuela@up.edu.mx
                    </td>

                    <td>Juan B. Calabrese
                        Universidad Panamericana
                        Facultad de Ingenier√≠a
                        Aguascalientes, M√©xico
                        bernardo.calabrese@up.edu.mx
                    </td>

                    <td>Josue Ortiz-Medina
                        Universidad Panamericana
                        Facultad de Ingenier√≠a
                        Aguascalientes, M√©xico
                        jortizm@up.edu.mx
                    </td>
                    
                    <td>Claudia N. S√°nchez
                        Universidad Panamericana
                        Facultad de Ingenier√≠a
                        Aguascalientes, M√©xico
                        cnsanchez@up.edu.mx
                    </td>
                </tr>
            </tbody>
        </table>

        <table>
            <tbody>
                <tr>
                    <td>
                        <b>   
                            <i>Abstract</i>‚Äî Convolutional Neural Networks (CNN) have
                            been used since the late 80‚Äôs. Nevertheless, until the 2000‚Äôs when
                            they begun to be popular for image classification tasks, thanks
                            to the improvements in computation performance of electronic
                            devices and new algorithms development. However, most of the
                            classifiers are oriented towards the processing of real-world
                            images. This document presents a CNN for hand-written
                            drawings recognition. The dataset consists of 710,000 images
                            that correspond to 71 different classes, each one with around
                            10,000 samples. The dataset was randomly divided into a
                            training set (80%) and a testing set (20%). The CNN classifier
                            achieved an accuracy of 84.79% for classifying the samples on
                            the testing set. The classification results showed perfect
                            identification for 10 classes, whereas 6 classes were poorly
                            classified. It is foreseen that the results presented here can fuel
                            applications where identification of hand-made drawings are
                            critical, such as neuropsychological tests.
                        </b>

                        <br>
                        <br>

                        <b style="text-align: center;">
                            <i>Keywords‚Äîdeep learning, convolutional neural network,
                            artificial intelligence, hand-written drawings</i>
                        </b>

                        <br>

                        <p style="text-align: center;">I. INTRODUCTION</p>

                        <p>Convolutional neural networks (CNN) have been applied
                        to visual applications since two decades ago at least [1], [2].
                        Nevertheless, besides some disperse reported applications, the
                        CNN remained relatively inactive until the middle of 2000‚Äôs,
                        when the developments on computational and data science,
                        along with the availability of diverse databases complemented
                        with advanced algorithms, lead to their rapid improvement
                        and use [3]. This work aims to contribute to the CNN
                        application to the classification of images based on hand-
                        written drawings, an area that has been somewhat unexplored.</p>

                        <p>Image classification can be defined as the process of image
                        categorization within one or more predefined classes, and is
                        one of the fundamental problems on computer vision and
                        related applications, such as localization, detection and
                        segmentation [4]‚Äì[7]. Image classification is an easy task for
                        humans, but a difficult one for a computer. Some of the typical
                        complications include object shape dependence on point-of-
                        observation and object variability [8]. Now, it can be
                        considered that image classification is a specific application
                        of pattern recognition and classification, which is generally
                        defined as the mapping over a big group of data, from which
                        smaller sub-groups or classes of data can be arranged
                        according to specific criteria. The pattern classification
                        process is basically composed of two parts: one,
                        characteristics extraction phase, and second, decision making
                        or classification phase. During the characteristics extraction
                        phase, the largest dimension is transformed into a set of
                        metrics, termed characteristics. These metrics thus represent 
                        intrinsic information about the patterns. One of the most
                        important features of CNNs is that they include both parts of
                        image classification, which means that they can perform the
                        features extraction and their classification, making their
                        implementation an easy task</p>.
                    </td>

                    <td>
                        <p>Currently, most of the classifiers and convolution
                        techniques aim to the processing of ‚Äúreal‚Äù images, i.e., images
                        from scenes in the 3D world we interact. This kind of
                        approaches in fact make these techniques suitable for robotic
                        and automation applications. Nevertheless, an increasing
                        amount of reports demonstrate the feasibility of using CNN
                        for hand-written images, with interesting advances in the field
                        of text and languages recognition [9]‚Äì[11]. Moreover, another
                        interesting and potential field where hand-written images
                        recognition and classification could have an important impact
                        is human health, given the potential of automated
                        interpretation of human writing or drawings from the
                        psychology point of view, or more precisely, in the
                        development of neuropsychological tests (NT) [12], [13].
                        Some works already report the use of machine learning (ML)
                        methodologies for improving the detection and diagnosis of
                        neurodegenerative diseases, describing the analysis of images
                        from specialized techniques such as computerized
                        tomography (CT), magnetic resonance imaging (MRI), etc.
                        [14], [15]. Fortunately, ML has also been explored for easier,
                        faster, and cheaper alternatives for tests and diagnosis by
                        means of hand-writing recognition and classification [12],
                        [16], [17]. NT represent a specialized area within clinic
                        psychology. Psychologists use the contextual information
                        from these tests for diverse kinds of evaluations (e.g., a child
                        with learning difficulties or a patient suffering of a
                        neurodegenerative disease). As it is previously mentioned,
                        there are several medical procedures such as CT, MRI or
                        positron-emission tomography (PET), which can show
                        graphically parts of the brain with physiological disorders.
                        Nonetheless, NT can reveal deep aspects of the whole brain
                        functioning. Unfortunately, some of these procedures are
                        lengthy, and composed of integral exams which include a
                        wide variety of subtests like the Halstead-Reitan [18] or Luria-
                        Nebraska [19]. One common feature of these tests is the use
                        of figures or specific forms of drawings. The scoring in these
                        tests are based in a variety of possible errors that the subjects
                        can commit while copying the figures, including missing
                        details, collision or superposition, inability of shape
                        completion (such as circles or squares), disproportionate sizes
                        and angles, or misorientations.</p>

                        <p>
                        Given that the score patterns within these subtests can be
                        used for specific cognitive failures identification, an
                        automated tool for hand-written drawings identification and
                        classification could provide a key advantage for the developing of NT based on ML. As mentioned, most of the
                        previous works on this topic have been oriented towards the
                        recognition and classification of characters, within the
                        language scope. Some seminal reports consider the
                        recognition of specific features in hand-written drawings [20],
                        which will be used as a basement for further develop a CNN
                        strategy for drawings classification. This work aims to provide
                        an efficient ML methodology for the classification of hand-
                        written sketches, which would provide a solid framework for
                        further developments towards their automatic interpretation.
                        The results demonstrate a high potential of CNN architectures
                        for drawings recognition and classification, which ultimately
                        could be applied to a wide variety of tests, including NT for
                        automated psychological evaluations. </p>
                    </td>
                </tr>
            </tbody>
        </table>

        <hr>

        <table>
            <tbody>
                <tr>
                    <td>
                        <p>The rest of the manuscript is organized as follows. Section
                        II shows the methodology of this work, starting with the data
                        description and then comes how the image classification was
                        performed. The results and discussion are presented in section
                        III, with the concluding remarks summarized in section IV.
                        </p>

                        <p style="text-align: center;">II. METHODOLOGY</p>

                        <p>A. Data description
                        The hand-drawing images used in the experiments were
                        taken from a dataset of Quick, Draw! a game that recollects
                        hand-written draws from millions of users around the world.
                        [21]. The dataset consists of 710,000 images that correspond
                        to 71 different classes each one with around 10,000 samples.
                        The list of classes are airplane, backpack, bat, beard, bee,
                        bird, bush, butterfly, cactus, car, cat, cloud, cow, dog, door,
                        ear, eye, eyeglasses, face, fish, flower, foot, garden, goatee,
                        grass, hand, hat, helmet, horse, house, jacket, leg, lightning,
                        moon, mountain, mouse, moustache, mouth, mushroom,
                        necklace, nose, ocean, palm_tree, pants, pig, rabbit, raccoon,
                        rain, rainbow, river, sailboat, screwdriver, sea_turtle, sheep,
                        shoe, shorts, skyscraper, smiley_face, snake, sock, squirrel,
                        star, sun, sweater, tent, The_Mona_Lisa, tornado, tree, t-shirt,
                        umbrella, and yoga. Figure 1 shows an example of the
                        images.</p>

                        <p>B. Image classification
                        As it is mentioned in Section I, CNNs have been
                        successfully used for image classification, and its power has
                        been probed in different contexts. For that reason, it was
                        decided to use a CNN for the classification of drawing
                        images.</p>

                        <p>CNNs are advanced networks due to the unidirectional
                        way they manage the information, from the inputs toward the
                        output. CNNs are, as it occurs in general with artificial-NN
                        (ANN), inspired in biological NN. The basic unit in an ANN
                        is a neuron, where a set of numerical inputs ùë•ùëñ are weighted
                        by a corresponding weight ùë§ùëñ . As can be seen in Equation 1.
                        The results are added, and finally, an activation function ùúë is
                        applied to yield the final result ùë¶ . The objective of the
                        activation function is to create nonlinear models.</p>

                        <img src="1.png" width="80%">

                        <img src="2.png" width="100%">

                    </td>

                    <td>
                        <p>The brain visual cortex, which consists of alternate
                        neuronal layers with different complexity, constitutes a
                        recurrent model for CNNs architectures. These architectures
                        can thus include specific layers for specific functions as
                        convolution, categorization, and sampling, grouped within
                        modules of one or more neuronal layers fully connected.
                        Then, a deep-CNN (DCNN) is built by stacking individual
                        modules. Figure 2 shows a typical architecture of a CNN used
                        for image classification; an image is provided as an input for
                        the DCNN, normally consisting of several convolution and
                        grouping stages. From that point, the outcomes become the
                        inputs for one or more fully connected NN layers. Finally, the
                        last fully connected NN layer is in charge of labeling the
                        classes.</p>

                        <img src="3.png" width="100%">

                        <p>The used CNN architecture is composed of 8 layers. Each
                        layer is described as follows:</p>
                        
                        <ul>
                            <li>1st: Convolutional layer, with 32 convolutional kernels of 3x3 size. The activation function is relu.</li>
                            <li>2nd: Max pooling layer, where the pool size is 2x2.</li>
                            <li>3rd: Convolutional layer, with 64 convolutional kernels of 3x3 size. The activation function is relu.</li>
                            <li>4th: Max pooling layer, where the pool size is 2x2.</li>
                            <li>5th: Convolutional layer, with 128 convolutional kernels of 3x3 size. The activation function is relu. We added 20% of dropout.</li>
                            <li>6th: Max pooling layer, where the pool size is 2x2</li>
                            <li>7th: Dense flatten layer with 128 neurons. The activation function is relu</li>
                            <li>8th: Dense flatten layer with 71 neurons. The activation function is softmax. We added 20% of dropout</li>
                        </ul>

                        <p>The total number of parameters of the CNN is 249,415.
                        The optimization algorithm used for training the CNN was
                        Adam [22], where the loss was defined as cross-entropy for
                        maximizing the accuracy. If ùë¶ and ùë¶ÃÇ are two vectors in ‚Ñùùëõ
                        that contain the real and predicted labels for ùëõ samples, the
                        accuracy is defined in Equations 2, where ùúë(‚ãÖ,‚ãÖ) is a function
                        that returns 1 if their inputs are equal or 0 otherwise. On the
                        other hand, considering ùë¶ and ùë¶ÃÇ as two matrices in ‚Ñùùëõùëò,
                        where ùëõ are the number of samples and ùëò the number of
                        classes. The matrix ùë¶ contains the real labels, the row ùëñ that
                        represent the sample ùëñ has only one 1 in the column that
                        corresponds to the real class, the rest of the cells in that row
                        are 0. The matrix ùë¶ÃÇ contains the predicted labels that
                        correspond to values between 0 and 1 that can be seen as the
                        likelihood to each sample belongs to every the class. Using
                        these matrices, the cross entropy is defined as Equation 3,
                        where ùëñ and ùëó represent the sample and the class, respectively.</p>

                        <img src="4.png" width="100%">

                        <p>For calculating the performance of the CNN classifier, the
                        images were randomly divided in 568,000 samples for
                        training (80%) and 142,000 (20%) for testing. In addition,
                        when training the CNN, the training set was randomly divided
                        into training (90%) and validation set (10%).</p>
                    </td>
                </tr>
            </tbody>
        </table>

        <hr>

        <table>
            <tbody>
                <tr>
                    <td>

                        <p style="text-align: center;">III. RESULTS AND DISCUSSION</p>

                        <p>The metric used for measuring the CNN performance is
                        accuracy. It counts the percentage of labels that the classifier
                        predicts correctly. Its value ranges from 0 to 1, being 1 a
                        perfect prediction.
                        </p>

                        <p>The CNN architecture described in section II achieved an
                        accuracy of 0.8479. Considering a multiclass problem of
                        hand-written drawings with a high number of classes, 71 in
                        this case, the obtained accuracy is quite relevant. The
                        confusion matrix is visually presented in Figure 3 where, as
                        it was expected, the diagonal matrix has high values, meaning
                        an almost perfect identification. The classes with high
                        confusion are the ones showed in Table 1. On the other hand, 
                        the classes that were predicted with an accuracy of 1.0 were
                        screwdriver, eye, rainbow, star, sun, cactus, the Mona Lisa,
                        tornado, backpack, and house. It is interesting to compare
                        these results with other related with hand-written drawings
                        recognition by CNN. For instance, accuracies as high as 89%
                        were achieved, but for a small number of classes for sketch-
                        made human expressions [20]. This highlights the importance
                        of this work, where 71 classes were recognized and classified.</p>

                        <p>Figure 4 shows some incorrect classification as a result of
                        the test process, but that can be considered as correct answers
                        since the images really seem to be what the classifier is
                        detecting on the image. This is shown in the confusion matrix,
                        as it can observed in Figure 3. Most of the wrong
                        classification cases are the result of various classes that have
                        high similarity with others. The CNN could be improved
                        reducing the classes that are similar on aspect (e.g. sweater,
                        t-shirt and jacket), choosing the one with the best accuracy
                        result, and reducing the number of classes from 5 to 10.</p>

                        <img src="9.png" width="100%">
                        <img src="5.png" width="100%">
                    </td>

                    <td>
                        <img src="6.png" width="100%">
                        <p>Figure 5 shows the convergence of the parameters'
                        optimization algorithm for CNN. It can be observed that the
                        results are reached around 200 epochs. In addition, it can be
                        verified that CNN is not overfitting the training set because
                        the loss of the validation set is lower than the training loss.</p>

                        <img src="7.png" width="100%">

                        <p style="text-align: center;">IV. CONCLUSIONS</p>

                        <p>This document presents a methodology based on deep
                        learning for classifying hand-written drawings. The dataset
                        was composed of 710,000 images of hand-written drawings of
                        71 different classes. The list of classes are airplane, backpack,
                        bat, beard, bee, bird, bush, butterfly, cactus, car, cat, cloud,
                        cow, dog, door, ear, eye, eyeglasses, face, fish, flower, foot,
                        garden, goatee, grass, hand, hat, helmet, horse, house, jacket,
                        leg, lightning, moon, mountain, mouse, moustache, mouth,
                        mushroom, necklace, nose, ocean, palm_tree, pants, pig,
                        rabbit, raccoon, rain, rainbow, river, sailboat, screwdriver,
                        sea_turtle, sheep, shoe, shorts, skyscraper, smiley_face, snake, sock, squirrel, star, sun, sweater, tent, The_Mona_Lisa,
                        tornado, tree, t-shirt, umbrella, and yoga. Each class contains
                        around 10,000 samples. The dataset was randomly divided
                        into training (80%) and testing set (20%).</p>

                        <p>The classifier was proposed as a CNN composed of 8
                        layers, with a total of 249,415 parameters. The optimization
                        algorithm was ADAM. The performance of the classifier was
                        an accuracy of 84.79%, being the following classes the one
                        with a perfect performance: screwdriver, eye, rainbow, star,
                        sun, cactus, the mona lisa, tornado, backpack, and house. On
                        the other hand, the classes with lower performance are dog,
                        sweater, raccoon, goatte, skyscraper, and cow. However, their
                        performance can be explained because the labels are related.
                        For example, the class dog was confused with horse and cow,
                        and, the class sweater was confused with t-shirt and jacket.</p>
                    </td>
                </tr>
            </tbody>
        </table>

        <hr>

        <table>
            <tbody>
                <tr>
                    <td>
                        <p>As future work, we plan to use the CNN presented in this
                        document for identifying key hand-written drawings to
                        interpret neuropsychological tests.</p>

                        <p style="text-align: center;">ACKNOWLEDGMENTS</p>

                        <p>The authors would like to acknowledge the computational
                        facilities of the Faculty of Engineering, Universidad
                        Panamericana campus Aguascalientes.</p>

                        <p style="text-align: center;">REFERENCES</p>
                        
                        <p>[1] Q. Z. Wu, Y. Le Cun, L. D. Jackel, and B. S. Jeng, ‚ÄúOn-
                        line recognition of limited-vocabulary Chinese character
                        using multiple convolutional neural networks,‚Äù in
                        Proceedings - IEEE International Symposium on Circuits
                        and Systems, 1993, vol. 4, pp. 2435‚Äì2438. <br>
                        [2] D. Wei, B. Sahiner, H. P. Chan, and N. Petrick,
                        ‚ÄúDetection of masses on mammograms using a
                        convolution neural network,‚Äù in ICASSP, IEEE
                        International Conference on Acoustics, Speech and Signal
                        Processing - Proceedings, 1995, vol. 5, pp. 3483‚Äì3486. <br>
                        [3] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù
                        Nature, vol. 521, no. 7553, pp. 436‚Äì444, May 2015. <br>
                        [4] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional
                        networks for large-scale image recognition,‚Äù in 3rd
                        International Conference on Learning Representations,
                        ICLR 2015 - Conference Track Proceedings, 2015. <br>
                        [5] M. Moetesum, O. Zeeshan, and I. Siddiqi, ‚ÄúMulti-object
                        sketch segmentation using convolutional object
                        detectors,‚Äù in Tenth International Conference on Graphics
                        and Image Processing (ICGIP 2018), 2019, p. 110.<br>
                        [6] A. Karpathy and L. Fei-Fei, ‚ÄúDeep Visual-Semantic
                        Alignments for Generating Image Descriptions,‚Äù 2015. <br>
                        [7] D. C. C. Cires¬∏an, U. Meier, J. Masci, L. M. Gambardella,
                        and J. ¬® Urgen Schmidhuber, ‚ÄúFlexible, High Performance
                        Convolutional Neural Networks for Image Classification,‚Äù
                        Jun. 2011. <br>
                        [8] M. A. Ranzato, F.-J. Huang, Y.-L. Boureau, and Y.
                        Lecun, ‚ÄúUnsupervised Learning of Invariant Feature
                        Hierarchies with Applications to Object Recognition.‚Äù <br> 
                        [9] J. A. S√°nchez, V. Romero, A. H. Toselli, M. Villegas, and
                        E. Vidal, ‚ÄúA set of benchmarks for Handwritten Text
                        Recognition on historical documents,‚Äù Pattern Recognit.,
                        vol. 94, pp. 122‚Äì134, Oct. 2019.<br>
                        [10] M. Rajalakshmi, P. Saranya, and P. Shanmugavadivu,
                        ‚ÄúPattern Recognition-Recognition of Handwritten
                        Document Using Convolutional Neural Networks,‚Äù in
                        IEEE International Conference on Intelligent Techniques
                        in Control, Optimization and Signal Processing, INCOS
                        2019, 2019. <br>
                        [11] C. Tirkaz, B. Yanikoglu, and T. Metin Sezgin, ‚ÄúSketched
                        symbol recognition with auto-completion,‚Äù Pattern
                        Recognit., vol. 45, no. 11, pp. 3926‚Äì3937, Nov. 2012. <br></p>
                    </td>

                    <td>
                        <p>[12] P. Khatamino, I. Canturk, and L. Ozyilmaz, ‚ÄúA Deep
                        Learning-CNN Based System for Medical Diagnosis: An
                        Application on Parkinson‚Äôs Disease Handwriting
                        Drawings,‚Äù in 2018 6th International Conference on
                        Control Engineering and Information Technology, CEIT
                        2018, 2018. <br>
                        [13] H. Bin Nazar et al., ‚ÄúClassification of Graphomotor
                        Impressions Using Convolutional Neural Networks: An
                        Application to Automated Neuro-Psychological Screening
                        Tests,‚Äù in Proceedings of the International Conference on
                        Document Analysis and Recognition, ICDAR, 2017, vol.
                        1, pp. 432‚Äì437.<br>
                        [14] I. R. R. Silva, G. S. L. Silva, R. G. De Souza, W. P. Dos
                        Santos, and R. A. A. De Fagundes, ‚ÄúModel Based on
                        Deep Feature Extraction for Diagnosis of Alzheimer‚Äôs
                        Disease,‚Äù in Proceedings of the International Joint
                        Conference on Neural Networks, 2019, vol. 2019-July. <br>
                        [15] E. Yagis, A. G. S. De Herrera, and L. Citi,
                        ‚ÄúGeneralization Performance of Deep Learning Models in
                        Neurodegenerative Disease Classification,‚Äù in
                        Proceedings - 2019 IEEE International Conference on
                        Bioinformatics and Biomedicine, BIBM 2019, 2019, pp.
                        1692‚Äì1698. <br>
                        [16] S. Impedovo, ‚ÄúMore than twenty years of advancements
                        on Frontiers in handwriting recognition,‚Äù Pattern
                        Recognit., vol. 47, no. 3, pp. 916‚Äì928, Mar. 2014.<br>
                        [17] M. A. El-Yacoubi, S. Garcia-Salicetti, C. Kahindo, A.-S.
                        Rigaud, and V. Cristancho-Lacroix, ‚ÄúFrom aging to early-
                        stage Alzheimer‚Äôs: Uncovering handwriting multimodal
                        behaviors by semi-supervised learning and sequential
                        representation learning,‚Äù Pattern Recognit., vol. 86, pp.
                        112‚Äì133, Feb. 2019. <br>
                        [18] R. M. Reitan, ‚ÄúWard Halstead‚Äôs contributions to
                        neuropsychology and the Halstead‚ÄêReitan
                        neuropsychological test battery,‚Äù J. Clin. Psychol., vol.
                        50, no. 1, pp. 47‚Äì70, 1994. <br>
                        [19] C. J. Golden, J. J. Sweet, and J. A. Moses, ‚ÄúRelationship
                        of the Halstead-Reitan Neuropsychological Battery to the
                        Luria-Nebraska Neuropsychological Battery A SPECT
                        Exploratory Analysis of Differentiating Mania
                        Symptomology Severity View project,‚Äù Artic. J. Consult.
                        Clin. Psychol., 1981. <br>
                        [20] M. Moetesum, T. Aslam, H. Saeed, I. Siddiqi, and U.
                        Masroor, ‚ÄúSketch-based Facial Expression Recognition
                        for Human Figure Drawing Psychological Test,‚Äù in
                        Proceedings - 2017 International Conference on
                        Frontiers of Information Technology, FIT 2017, 2017,
                        vol. 2017-Janua, pp. 258‚Äì263. <br>
                        [21] Google, ‚ÄúQuick, Draw! The Data.‚Äù [Online]. Available:
                        https://quickdraw.withgoogle.com/data. [Accessed: 21-
                        Jun-2020]. <br>
                        [22] D. P. Kingma and J. Ba, ‚ÄúAdam: A Method for Stochastic
                        Optimization,‚Äù Dec. 2014.</p>
                    </td>
                </tr>
            </tbody>
        </table>



    </body>
</html>
